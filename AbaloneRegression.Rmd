---
title: "Predicting Abalone Age Through Ridge Regression"
author: "Michael Szczepaniak"
date: 'Revised: January 2016'
output:
  pdf_document:
    toc: yes
  html_document:
    number_sections: yes
    toc: yes
---

```{r echo=FALSE, eval=FALSE, message=FALSE}
# setwd("D:/dev/AbaloneRegression")       # workstation
# setwd("C:/data/dev/AbaloneRegression")  # laptop
```

---
\pagebreak

# Introduction
There are a number of packages in R, Python, and other languages that can perform linear regression analysis (LRA) on a given dataset.  While the use of these packages saves a data scientist or analysis quite a bit of time, it is important to do these from scratch every now and then to reenforce the concepts behind this most fundamental of machine learning algorithms.  By doing so, we deepen our intuitions that allow us to make better use of the available tools.

In this project, an LRA was performed on a dataset of characteristics (features) related to the number of abalone rings which was treated as the independent or response variable.  This analysis was performed in two ways.  First, it was done "from scratch" using nothing but base R language constructs such as matrix operations.  Second, the analysis is duplicate used the caret package.

The analysis starts with a quick exploratory data analyisis (EDA) which begins with standardizing the indepependent variables (features) before constructing a matrix of scatter plots in order to gain a high level understanding of the relationships between feature pairs. The features analyzed were: Sex (Male, Female, or Infant), Length, Diameter, Height, Whole weight, Shucked weight, Viscera weight, and Shell weight.

Following the pairs plot, the construction of the linear model was initiated by randomly partitioning the data into training and test sets. Following the data partitioning, the coefficients for the linear model were calculated for various ridge regression parameters ($\lambda$) on the training set.  The $\lambda$ which minimized the root mean square error (RMSE) was determined and then used in the model to make some predictions.

The parameters for the optimized linear model was as follows:

```
##                 weight      mean    std dev
## bias         9.9138241        NA         NA
## male         0.4545017 0.3626571 0.48083896
## female       0.3812974 0.3150808 0.46461756
## Length       0.1710038 0.5244749 0.11957535
## Diameter     0.9011211 0.4084455 0.09864034
## Height       0.4174376 0.1398683 0.04250690
## Whole Wgt    3.7370421 0.8300963 0.48661766
## Shucked Wgt -4.0358844 0.3593069 0.21908767
## Viscera Wgt -1.0573166 0.1807382 0.10868213
## Shell Wgt    1.4083556 0.2400070 0.13945367
```

The optimized ridge regression parameter $\lambda$ was found to be 0.8. The mean and the std dev columns above were used in the standardization which will be explained in detail later.

# Background Information
## What are Abalone?
Abalone are marine snails. Abalone shells have a low and open spiral structure, and are characterized by several respiratory holes in a row near the shell's outer edge. The innermost layer of the shell is composed of nacre or mother-of-pearl, which in many species is highly iridescent, giving rise to a range of strong and changeable colors which make them attractive to humans as decorative objects [[1]](http://en.wikipedia.org/wiki/Abalone).

The abalone of the Northwest's Puget Sound are a delicacy in Asia, prized for their meat and beautiful shell. As a result they were poached nearly to extinction in the early 1990s, but with a little help from scientists, the wild abalone is slowly recovering [[2]](http://loe.org/shows/segments.html?programID=15-P13-00031&segmentID=3).

## Why Model Rings
Rings in abalone correspond to their age, similart to rings in trees.  The age of abalone would be of interest to marine biologists as well as abalone farmers for a variety of reasons. Although other methods exist [[3](http://www.researchgate.net/profile/Craig_Mundy/publication/201169638_Determining_age_and_growth_of_abalone_using_stable_oxygen_isotopes_a_tool_for_fisheries_management/links/004635387d6171b3f3000000.pdf)], the method described on the data repository page for determining the age of abalone involves cutting the shell through the cone, staining it, and counting the number of rings through a microscope which is a destructive and time-consuming task [[4](https://archive.ics.uci.edu/ml/datasets/Abalone)].

In order to make the task of predicting age faster and easier, other measurements, which are easier to obtain, were investigated to predict the age [[4](https://archive.ics.uci.edu/ml/datasets/Abalone)].

# Data Preparation
## Reading the Data

The data was first obtained from the UCI repository[[4](https://archive.ics.uci.edu/ml/datasets/Abalone)] and then read into R by running:

```{r}
abalone <- read.table("abalone.data", sep=',')
names(abalone) <- c("Sex", "Length", "Diameter", "Height", "Whole Wgt",
                    "Shucked Wgt", "Viscera Wgt", "Shell Wgt", "Rings")
```
When running this command, one must make sure that the working directory is set to the same directory where the data file resides.

## Converting Catagorical Data

The first column of the original data [[4](https://archive.ics.uci.edu/ml/datasets/Abalone)] was labeled **Sex** and was populated with the categorical values **M**, **F**, and **I** corresponding to males, females and infants respectively.  In order to facilitate inclusion of the **Sex** variable into the model, these categorical values were converted to numerical values.  This was done by replacing the original column with two new columns.  The first new column was labeled **M** for male and the second was labeled **F** for female.  Males were deignated by putting a 1 in the **M** column and a 0 in the  **F** column.  Females were designated by putting a 0 in the **M** column and a 1 in the **F** column.  Infants were designated by a 0 in both the **M** and **F** columns.  

The following code was used to convert the **Sex** (V1) column:

```{r convertSex, cache=TRUE}
convertSex <- function(data.df = abalone) {
    library(dplyr)
    male <- (data.df[, 1] == 'M') * 1
    female <- (data.df[, 1] == 'F') * 1  # if I, left as 0
    # leave out the first row and prepend the two new col's
    converted <- cbind(male, female, data.df[, 2:length(data.df)])
    return(converted)
}
```

## Data Partitioning
As is standard practice, the model was constructed using randomly selected portions of the dataset.  The portion used to create the model is referred to as the $\textit{training set}$ while the remaining portion which is used to test and validate predictions is referred to as the $\textit{testing set}$.

During the analysis, many iterations based on different randomly selected training and test sets had to be constructed.  The following two R functions were frequently called to accomplish this task:

```{r}
# Returns a randomly selected subset of rows from the original input dataframe.
#
# orig - original (complete) dataset (dataframe), assumes that samples
#        are stored in as rows
# fraction - fraction of the original dataset used for training
# randomSeed - integer used to set the seed for the random number generator
#              (used to provide reproducibility)
trainSetRows <- function(orig, fraction=0.8, randomSeed=71198) {
  set.seed(randomSeed)
  randorder <- sample(nrow(orig))  # create set of randomly selected rows
  nTrain <- round(nrow(orig)*fraction) # calc # of rows in training set
  trainRows <- randorder[1:nTrain]  # 1st nTrain rows in rand order
  return(trainRows)
}

# Returns a vector of integers corresponding to row numbers of the test set
# which are assumed to be all the rows NOT in the training  set.
#
# allRows - a vector containing all the row numbers in the data table
# trainRows - the vector of training rows returned from the trainSetRows function
testSetRows <- function(allRows, trainRows) {
  testRows <- setdiff(allRows, trainRows) # take away train rows leaves test rows
  return(testRows)
}
```


# Regressions Analysis
## The Linear Model
### Definition of the Linear Model
The model used to describe the data is as follows:

(@basemodel) $R = w_0 + \sum_{i=1}^{9}{w_i}{x_i}$

In equation (@basemodel), $R$ is the number of rings the abalone has, the $w$'s are the weights (our fitted parameters) and the $x_i$'s are the independent variables.  The independent variables can be described as: ${x_1}$ = M, ${x_2}$ = F, ${x_3}$ = Length, ${x_4}$ = Diameter, ${x_5}$ = Height, ${x_6}$ = Whole weight, ${x_7}$ = Shucked weight, ${x_8}$ = Viscera weight, and ${x_9}$= Shell weight.

### $\lambda$ Parameterization

The goal was to fit the $w$ vector described in (@basemodel), in such a way as to minimize the residual sum of squares (RSS) parameterized by the ridge regression factor $\lambda$ which penalizes large coefficients.  Since minimzing a constant times RSS is the same as minimizing RSS, the error which we want to minimize can be written in matrix form as:

(@rss) $\frac{1}{2}RSS = \frac{1}{2}\sum_{i=1}^{9}\left({t_n}-{\textbf{w}^T}{\textbf{x}_i}\right)^2 + \frac{1}{2}\lambda{\textbf{w}^T}{\textbf{w}}$

where $t_n$ are know target (R) values and the constant $\frac{1}{2}$ has been multiplied by both sides to make the result of differentiation cleaner.  Taking the gradient of (@rss), setting it equal to zero, and solving for the coefficients vector $\textbf{w}$, yields the following solution in matrix notation [[5](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf)]:

(@weightSolution) $\textbf{w} = \left({\textbf{X}^T}{\textbf{X} + \lambda{\textbf{I}}}\right)^{-1}{\textbf{X}^T}{\textbf{T}}$ 

The size of the independent variables matrix $\textbf{X}$ is $\left(n \times d+1 \right)$ where $n$ is the number of samples and $d+1$ is the number of independent variables in the $\textbf{w}$ vector which includes the $w_0$ bias term.  The dependent variables matrix of targets $\textbf{T}$ is a column vector of size $\left(n \times 1 \right)$.  The diagonal matrix $\lambda\textbf{I}$ is size $\left(d+1 \times d+1 \right)$.

### Calculating weights
In order to use the `solve()` function in R, we first need to rewrite (@weightSolution) in the following form:

(@weightSolution2) $\left({\textbf{X}^T}{\textbf{X} + \lambda{\textbf{I}}}\right)\textbf{w} ={\textbf{X}^T}{\textbf{T}}$ 

With equation (@weightSolution2), we now have an expression that we can pass to ```solve()``` to calculate the weight vector $\textbf{w}$ given parameter $\lambda$.  The function `llsMakeST` was written to accomplish this task with care taken to ensure that the $w_0$ bias term was not penalized by zeroing out the first term in the $\lambda{\textbf{I}}$ matrix as shown below.

```{r}
## llsMakeST (Linear Least Squares Make for a Singe Target) Returns a vector of
## the linear least square regression weights for a single target variable with
## the Ridge Regression penalty factored in.
## 
## For n = number of samples and d + 1 = number of coefficients including bias:
## X - n x d+1 coefficient matrix wrt the independent variables
## Y - n x 1 vector of values for the dependent target variable
## lambda (optional) - ridge regression (square weight) penalty factor
##                     (0 by default, regular unpenalized regression)
## 
## IMPORTANT USAGE NOTES:
## (1) Independent var's should NOT be standardized because it's done internally.
## (2) Samples w/ missing data need to be removed before running this function.
## (3) If lambda is not passed, straight (unpenalized) linear regression is done.
llsMakeST <- function(X, Y, lambda=0) {
    # if X or Y come in as a list, convert it to numeric
    X <- as.matrix(apply(X, 2, as.numeric))
    # get col means and std dev to use for standardization
    means <- matrix(apply(X, 2, mean), ncol(X), 1)
    stdevs <- matrix(apply(X, 2, sd), ncol(X), 1)
    N <- nrow(X)  # number of samples
    D <- ncol(X)  # number of parameters (dimensions)
    # standardize the train values
    Xs <- (X - matrix(rep(means, N), N, D, byrow=TRUE))/
          matrix(rep(stdevs, N), N, D, byrow=TRUE)
    
    Y <- as.matrix(apply(Y, 2, as.numeric))
    # account for the bias
    Xs1 <- cbind(1, Xs)
    colnames(Xs1)[1] <- "bias"
    # now solve for the weights, be sure not to penalize w0
    w <- solve((t(Xs1) %*% Xs1)+(lambda*diag(c(0,rep(1,ncol(Xs1)-1)))),
               t(Xs1) %*% Y)
    # bind the means and std dev's to be used for std'izing inputs for predictions
    w <- cbind(w, rbind(c(NA), means))
    w <- cbind(w, rbind(c(NA), stdevs))
    colnames(w) <- c("weight", "mean", "std dev")
    
    return(w)
}
```

The `llsMakeST` function returns a three column vector.  In the first column are the weights as defined by $\textbf{w}$.  The second and third columns of this matrix contain the means and standard deviations of the independent variable that were used in training the model. By training, we mean the portion of the date used to solve for $\textbf{w}$.

The second and third columns of the $\textbf{w}$ matrix are used to standardize the independent variables. Standardizing takes the following form:

(@standardizex) $\textbf{X}_s = \frac{(\textbf{X} - {\textbf{X}_{mean}})}{\textbf{X}_{stdev}}$

In equation (@standardizex), $\textbf{X}_s$ is the matrix of standardized inputs  (independent variables) and the matrices ${\textbf{X}_{mean}}$ and ${\textbf{X}_{stdev}}$ are the means and standard deviations of the inputs.  Standardization provides two advantages: (i) it allows us the ability to better account for significant difference between the regions of training and testing and (ii), it allows us to more easily compare the relative magnitude of the weights.

## Making Predictions
To make predictions from the `llsMakeST` weights, the `llsUseST`function was created:

```{r}
## This function returns a matrix of the predicted target values
## based on the weights determined in llsMakeST.
llsUseST <- function(weights, X) {
   X <- as.matrix(apply(X,2,as.numeric))
   # standardize X
   N <- nrow(X)  # number of samples
   D <- ncol(X)  # number of parameters of dimensions
   means <- weights[2:nrow(weights),2]
   stdevs <- weights[2:nrow(weights),3]
   # standardize the inputs used for prediction
   Xs <- (X - matrix(rep(means, N), N, D, byrow=TRUE))/
          matrix(rep(stdevs, N), N, D, byrow=TRUE)
   
   Xs1 <- cbind(1, Xs)  # account for the bias
   colnames(Xs1)[1] <- "bias"
   weights[1, 3] <- 1  # replace bias stdev to avoid a division by 0
   model <- Xs1 %*% weights[,1]  # calc the prediction of a single target
   return(model)
}
```

The `llsUseST` function outputs the vector **`model`** which contains the model predictions for the independent variables $\textbf{X}$ that were passed in.  These predictions could then be used to evaluate RMSE which in turn allowed us to explore how $\lambda$ and training partition impacted the models accuracy.  This will be explored in detail in the last part of the **Experiments** section which follows.

# Experiments
## A Quick Look at the Data
Before diving into the regression analysis, it's normally a good idea to take a look at the relationship between the variables - particularly the independent variables.  To accomplish this, the following R code utilizing the *pairs* function was used:
```{r}
panel.hist <- function(x) {
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5))
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <-y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col="cyan")
    dev.copy2eps(file="scatter2.eps")
}

panel.cor <- function(x,y,digits=4, prefix="", cex.cor) {
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0,1,0,1))
    r <- abs(cor(x,y))
    txt <- format(c(r, 0.123456789), digits=digits)[1]
    txt <- paste(prefix, txt, sep="")
    if(missing(cex.cor)) cex <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex=cex*r)
}

diag_hist <- function(y) {
    pairs(y, diag.panel=panel.hist, upper.panel=panel.cor)
}
```
This code produced the chart shown in Figure 1.

In Figure 1, the upper triangular plots were just a transposed version of the charts in the lower trinagular region, so it seemed reasonable to put somethin more meaningful in this region.  In this case, the correlation coefficient between *x* (horizontal axis) *y* (vertical axis) variables were selected.  Similarly, histograms were chosen for the diagonals.

Because including independent variables in the regression model which have strong dependencies with other variables is an undesirable quality, we look for these dependencies in Figure 1.  In doing so, we can observe some strong dependencies between variables such as *Length* and *Diameter*.  In the the **Results** section, we delve into this in more detail.  There also appears to be some dependencies between the different kinds of weights (Shucked, etc.), but the thickness of the scatter is greater than the thickness between *Length* and *Diameter*.

```{r plot1, echo=FALSE, message=FALSE}
abalone.converted <- convertSex()
diag_hist(abalone.converted)
```
Figure 1: Scatter Plot Matrix of Abalone Data

## Most (MSFs) & Least (LSFs) Significant Factors
For convenience, the table shown in the **Introduction** section is shown here again.

```{r echo=FALSE, message=FALSE}
train.rows <- trainSetRows(abalone.converted)
X <- as.matrix(abalone.converted[train.rows,-10])
Y <- as.matrix(abalone.converted[train.rows, 10])
abalone.wgts <- llsMakeST(X, Y)
abalone.wgts
```

Now we can reap the benefit of standardizing the inputs.  Since the inputs were standardized, the importance of the weights can now easily be seen.  Based on the relative magnitude of the weights, the three most significant factors are (in order of importance): bias, Whole weight and Shucked weight.

The three least significant factors are: Length, F, and M.  In fact, if we removed these three factors from our model, it does not change noticeably.  The following R code was used to remove the aforementioned factors and generate the plots shown in Figure 2.

```{r}
par(mfrow = c(1,2))
tp <- 0.5  # set training partition to be half of the dataset
trainRows <- trainSetRows(abalone, tp)
testRows <- testSetRows(seq(1:nrow(abalone)),trainRows)
Xtrain <- abalone.converted[trainRows, 1:9]
Xtrain <- as.matrix(apply(Xtrain, 2, as.numeric))
Xtest <- abalone.converted[testRows, 1:9]
Xtest <- as.matrix(apply(Xtest, 2, as.numeric))
Ttrain <- abalone.converted[trainRows, 10]
Ttrain <- as.matrix(Ttrain)
Ttest <- abalone.converted[testRows, 10]
Ttest <- as.matrix(Ttest)
lambda <- 0.8

w <- llsMakeST(Xtrain, Ttrain, lambda)
Tpred <- llsUseST(w, Xtest)
plot(Ttest, Tpred, main=c("All the Parameters",
     "(training partition 0.5)"), xlab="Rings (actual)",
     ylab="Rings (predicted)", ylim=c(0, 25))
abline(coef=c(0,1), col="red")
legend("bottomright",c("Test vs. Prediction", "Test = Prediction"),
        col=c("black","red"), lty=c(-1,1), pch=c(1,-1), merge = TRUE)

Xtrain <- Xtrain[, 4:9]
Xtest <- Xtest[, 4:9]
w <- llsMakeST(Xtrain, Ttrain, lambda)
Tpred <- llsUseST(w, Xtest)
plot(Ttest, Tpred, main=c("LSF Variables Removed",
     "(training partition 0.5)"), xlab="Rings (actual)",
     ylab="Rings (predicted)", ylim=c(0, 25))
abline(coef=c(0,1), col="red")
legend("bottomright", c("Test vs. Prediction", "Test = Prediction"),
       col=c("black", "red"), lty=c(-1,1), pch=c(1,-1), merge = TRUE)
```
Figure 2: Model with and without the three LSFs. $\lambda = 0.8$

The value of $\lambda$ for the plots in Figure 2 will be explained later on in this section.

## Effects of Training Partition and $\lambda$
To explore the effects of the training partition size, many plots were generated over a span of training partitions ranging from half a percent (0.005) to 90% (0.9).  From a practical standpoint, the area of most interst was when training partitions were relatively small.  This is because in reality, good data to build models upon if frequently scarce.

A typical looking plot in the low training partition region would look like the plot in Figure 3.

The code used to generate the plot in Figure 3 is shown here.  The functions 'llsMake' and 'llsUse' described earlier are utilized.

```{r}
# creates a 3 column dataframe - 1st col is lambdas, 2nd col is RMSE of
# the training data, 3rd col is RMSE of testing data
# should not be standardized before passing them in because this is done
# within the llsMake and llsUse functions that are called within this
# function.
# Xtrain & Xtest are the training and test inputs respectively
# Ytrain & Ytest are the training and test target outputs respectively
# lambdas is a vector of lambdas which we want to test
rmseVsLambda <- function(Xtrain, Xtest, Ytrain, Ytest, lambdas) {
    
    # initialize results vector which will be built in the for loop
    results <- c()
    # calc weights, calc model prediction, and calc rmse for each lambda
    # append the results vector after each loop pass
    for(lam in lambdas) {
       # calculate the weights for a given lambda
       w <- llsMakeST(Xtrain, Ytrain, lam)
       pred <- llsUseST(w, Xtest)
       results <- rbind(results, c(lam, sqrt(mean((llsUseST(w, Xtrain) - Ytrain)^2)),
                                        sqrt(mean((pred - Ytest)^2))))
    }
    #results <- as.matrix(results)
    return(as.data.frame(results))
}

trainFrac <- 0.04
trainIndices <- trainSetRows(abalone.converted, fraction = trainFrac)
testIndices <- testSetRows(1:length(abalone$Rings), trainIndices)
x.train <- abalone.converted[trainIndices, 1:9]
x.test <- abalone.converted[testIndices, 1:9]
y.train <- matrix(data = abalone.converted[trainIndices, 10],
                  nrow = length(abalone.converted[trainIndices, 10]))
y.test <- matrix(data = abalone.converted[testIndices, 10],
                 nrow = length(abalone.converted[testIndices, 10]))
# lambda.values <- c(0.05, seq(0.1, 1, by=0.1), seq(2, 10, by=0.5), 10:25)
lambda.values <- seq(0.01 , 10, by=0.05)
results <- rmseVsLambda(x.train, x.test, y.train, y.test, lambda.values)
names(results) <- c("lambda", "train", "test")

plotFigure3 <- function(results) {
    legpos <- "bottomright"
    ylim <- range(results[, 2:ncol(results)])  # find the RMSE range
    # override the ylim to squeeze range a bit...
#     ylim[2] <- 2.75
#     ylim[1] <- 1.75
    #label <- c("RMSE vs. Lambda","training partition",tparts[1,i])
    matplot(results[,1], results[, c("train", "test")], type="b",
            xlab=expression(lambda), ylab="RMSE", ylim=ylim,
            main=paste0("RMSE vs. Lambda,", " Training Set = ", trainFrac))
    legend(legpos,c("Train", "Test"),
           col=c("black","red"),pch=c("1", "2"), lty=c(1, 2))
}

plotFigure3(results)

```
Figure 3. Typical model behavior with sparse training data

In order to explore a range of training partitions, the following code was written to do a number of iterations on each training partition. This needed to be done so that we could average the results because the $RMSE$ vs. $\lambda$   curve could vary significantly within the same training partition due to the random nature of the selection of members for the training and test partitions.

In order to accomplish this task, the following R function was written. This function was built from
functions which have been described earlier:

```{r}
# Analyzes the effect of lambdas and training partition fractions (TPFs) on RMSE
# std_data is the matrix containing the input and output param's,
# lambdas - a vector of lambdas to test over
# partReps - number of repetitons to be average at every (lambda, TPF)
# outputs - a matrix were the 1st col. are the lambdas that were passed in
# two col's are then added fro every training partition: one for the RMSE
# (training) and one for the RMSE (testing).
partition_exps <- function(std_data, lambdas, partReps=30) {
    init <- matrix(0, length(lambdas), 2)  # used to initialize resultsum
    # initialize the training set fractions
    tsetfracs <- c(0.005,0.01,0.02,0.04,0.06,0.08,0.1,0.2,0.3)
    # initialize the overall results with just the lambdas
    resultall <- matrix(c(lambdas),length(lambdas),1)  # 1 col matrix of lambdas
    colnames(resultall)[1] = "lambda"
    # outer loop: training set partitions
    for(tsf in tsetfracs) {
        # cat("\ntsf =", tsf, "\n")
        colnames(init)[1:2] <- c(paste("RMSE(trn)",tsf),
                                 paste("RMSE(tst)",tsf))
        resultsum <- init
        # inner loop: 200 random iterations on each training set fraction (TSF)
        for(parts in 1:partReps) {
            # cat(", parts =", parts)
            # establish the random set of training rows based on the TSF
            trainRows <- trainSetRows(std_data, tsf)
            # the test set was everything that wasn't the training set...
            testRows <- testSetRows(seq(1:nrow(std_data)),trainRows)
            # load the independent var's for training and test sets
            Xtrain <- std_data[trainRows, 1:9]
            Xtest <- std_data[testRows, 1:9]
            # load the target values for training and test sets
            Ttrain <- std_data[trainRows, 10, drop=FALSE]
            Ttest <- std_data[testRows, 10, drop=FALSE]
            # now calc. the results...
            results <- rmseVsLambda(Xtrain, Xtest, Ttrain, Ttest, lambdas)
            # tally our results
            resultsum <- resultsum + results[,2:3]
        }  # close inner loop
        
        # component division to turn resultsum into averages
        resultavg <- resultsum / partReps
        resultall <- cbind(resultall, resultavg)  # partition result for each lambda
    }  # close outer loop
    
    resultall
}

results <- partition_exps(abalone.converted, lambda.values, 10)
```

The plotting of all 9 of these graphs together is shown in Figure 4. These graphs were scaled on both x and y axis so that they could be readily compared.  The code used to generate this figure is shown below:

```{r fig.width=6, fig.height=6}
## Plots the effect of training partition on lambda
plot_lambda_study <- function(results) {
tparts <- matrix(c(0.005,0.01,0.02,0.04,0.06,0.08,0.1,0.2,0.3,seq(2,18,by=2)),
                 2,9,byrow=TRUE)
   par(mfrow = c(3,3))  # set up output of 3x3 matplots
   for(i in 1:9) {
      legpos <- "bottomright"
      if(i==1)
         legpos <- "center"
      traincol <- tparts[2,i]
      testcol <- traincol + 1
      ylim <- range(results[,2:ncol(results)])  # find the RMSE range
      # override the ylim to squeeze range a bit...
      ylim[2] <- 2.75
      ylim[1] <- 1.75
      #label <- c("RMSE vs. Lambda","training partition",tparts[1,i])
      matplot(results[,1],as.matrix(results[,traincol:testcol]),type="b",
         xlab=expression(lambda),
         ylab="RMSE",
         ylim=ylim,
         main=paste("Training Partition", tparts[1,i]))
      legend(legpos,c("Train", "Test"),
         col=c("black","red"),
         pch=c("1","2"), lty=c(1,2))
   }
   # dev.copy2eps(file="tparts_study3x3a.eps")
}

plot_lambda_study(results)
```


# Results

# A Personal Curiosity: Bias and Training Partition

# Discussion

# References
[1] Wikipedia - https://en.wikipedia.org/wiki/Abalone  
[2] Living On Earth - http://loe.org/shows/segments.html?programID=15-P13-00031&segmentID=3  
[3] Stable oxygen method - http://www.researchgate.net/profile/Craig_Mundy/publication/201169638_Determining_age_and_growth_of_abalone_using_stable_oxygen_isotopes_a_tool_for_fisheries_management  
[4] UCI data repository (data source) - https://archive.ics.uci.edu/ml/datasets/Abalone  
[5] [The Elements of Statistical Learning - 2nd Edition, page 64 http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf  
[9] Rings not annual - http://www.publish.csiro.au/?paper=MF9921215